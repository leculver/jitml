{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE Selection\n",
    "\n",
    "In this notebook we will be exploring CSE selection.  We will be comparing a few different mechanisms for selecting CSEs and how they do relative to each other.  We will start by comparing the current JIT heuristic for CSE selection to both random CSE selection and the perfscore we get from choosing no CSEs at all.  We will then revisit how our initial, simple reinforcement learning model did.  Finally, we will build a classification model using neural networks.\n",
    "\n",
    "For our neural network approach, we will start with a basic model:  Just picking *individual* CSEs and trying to predict which CSEs will result in a positive or negative perfscore.  This isn't quite the general case of choosing a sequence of CSEs to JIT (in fact we found this simple single-cse model does not do that well for this task).  We will explore trying to generalize to choosing the full CSE sequence in future work.\n",
    "\n",
    "Before getting started, we need to set up some parameters and define a function to calculate `geomean` which we'll use to compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 13:45:52.425166: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-19 13:45:52.457500: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-19 13:45:52.457534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-19 13:45:52.458348: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-19 13:45:52.463531: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 13:45:53.112363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# Constants and parameters\n",
    "\n",
    "# Update these values to point at your local build\n",
    "MCH_FILE = \"~/git/runtime/artifacts/spmi/mch/43854594-cd60-45df-a89f-5b7697586f46.linux.x64/libraries_tests_no_tiered_compilation.run.linux.x64.Release.mch\"\n",
    "CORE_ROOT = \"~/git/runtime/artifacts/bin/coreclr/linux.x64.Checked/\"\n",
    "\n",
    "# At what perf_score would we want to select a CSE?  We don't want to select any CSE which\n",
    "# is >0.0, as we are creating new temporaries for no value.\n",
    "# I've arbitrarily selected this minimum perf_score improvement for a \"successful\" CSE.\n",
    "CSE_SUCCESS_THRESHOLD = -5.0\n",
    "\n",
    "# We will only retain features which have a correlation of at least 1% with the change in\n",
    "# perf_score.  This is to reduce the number of features we have to consider.  Selecting a\n",
    "# value as high as 15% would still be reasonable, but since there are so few features we\n",
    "# can afford to allow a lower threshold.\n",
    "CORRELATION_THRESHOLD = 0.01\n",
    "\n",
    "# The reinforcement learning trained model to compare against.\n",
    "RL_MODEL = \"../models/rl/ppo.zip\"\n",
    "MODEL_ALGORITHM = 'PPO'\n",
    "\n",
    "# Single CSE Classification Model parameters\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'binary_crossentropy'\n",
    "METRICS = ['accuracy']\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Setup and imports\n",
    "\n",
    "# resolve '~' to home directory\n",
    "import os\n",
    "MCH_FILE = os.path.expanduser(MCH_FILE)\n",
    "CORE_ROOT = os.path.expanduser(CORE_ROOT)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add the parent directory to the path so that we can import the jitml module\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from jitml import SuperPmiContext, MethodKind, SuperPmiCache, SuperPmi, get_individual_cse_perf, JitType\n",
    "\n",
    "ctx = SuperPmiContext(mch=MCH_FILE, core_root=CORE_ROOT)\n",
    "cache : SuperPmiCache = ctx.create_cache()\n",
    "spmi : SuperPmi = ctx.create_superpmi()\n",
    "spmi.start()\n",
    "\n",
    "def calculate_geomean(scores, baselines):\n",
    "    ratios = [score / baseline for score, baseline in zip(scores, baselines)]\n",
    "    log_ratios = np.log(ratios)\n",
    "    mean_log_ratios = np.mean(log_ratios)\n",
    "    geomean = np.exp(mean_log_ratios)\n",
    "    return geomean\n",
    "\n",
    "def print_difference(scores, baseline, name : str, baseline_name : str):\n",
    "    if not scores or not baseline:\n",
    "        print(f\"No data for {name} or {baseline_name}\")\n",
    "        return\n",
    "\n",
    "    same_as_no_cse = 0\n",
    "    bad_change = []\n",
    "    good_change = []\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] is None:\n",
    "            continue\n",
    "\n",
    "        diff = scores[i] - baseline[i]\n",
    "        if np.isclose(diff, 0):\n",
    "            same_as_no_cse += 1\n",
    "        elif diff < 0:\n",
    "            good_change.append(diff / scores[i] * 100.0)\n",
    "        else:\n",
    "            bad_change.append(diff / scores[i] * 100.0)\n",
    "\n",
    "    print(f\"Geomean of {name} vs {baseline_name}: {calculate_geomean(scores, baseline):.2f}\")\n",
    "    print()\n",
    "    print(f\"% of time same score:          {100 * same_as_no_cse / len(scores):.2f}%\")\n",
    "    print(f\"% of time {baseline_name} is better:    {100 * len(bad_change) / len(scores):.2f}%\")\n",
    "    print(f\"% of time {name} is better: {100 * len(good_change) / len(scores):.2f}%\")\n",
    "    print()\n",
    "    print(f\"Average improvement when {name} is better: {np.mean(good_change):.2f}%\")\n",
    "    print(f\"Average degradation when {baseline_name} is better:     {np.mean(bad_change):.2f}%\")\n",
    "    print()\n",
    "\n",
    "class TqdmCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self._progress = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._progress = tqdm(total=self.epochs, desc='Epochs', unit='epoch', ncols=120)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self._progress.update(1)\n",
    "        self._progress.set_postfix(acc=logs.get('accuracy'), val_acc=logs.get('val_accuracy'))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self._progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How often are CSEs a net benefit to enable?\n",
    "\n",
    "Let's first take a quick look at how often enabling an individual CSE is a good decision or not.  To do this, we've generated the `individual_cse_perf` dataset.  This dataset was generated by first JIT'ing every method with no CSEs enabled as a baseline, then taking every viable CSE and JIT'ing it individually (no other CSEs enabled) then comparing the resulting perfscore to the baseline.\n",
    "\n",
    "This tells us a simplified story of what we are looking for:  Whether or not each individual CSE is a net improvement or detriment to overall perfscore when we enable it (and by what magnitude).  This is not the complete picture, since each CSE decision will affect every other CSE decision, but this is a good point for understanding.\n",
    "\n",
    "We will query this dataset for all viable CSEs.  We will count the number of times each individual CSE was a net benefit to perfscore as both a raw number and as a percentage.  We will then take a look at how often the JIT heuristic selected individual CSEs.\n",
    "\n",
    "The result seems to be that each CSE decision is a coinflip:  Around 46% of the time, enabling a CSE is a good decision.  The heuristic seems to be slightly more conservative than that, only selecting only 32% of viable CSE candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg good CSEs:   1.9211512105984467\n",
      "Avg good CSEs %: 46.08%\n",
      "\n",
      "Avg heuristic CSEs:   1.5503121668950814\n",
      "Avg heuristic CSEs %: 31.88%\n"
     ]
    }
   ],
   "source": [
    "individual_cse = get_individual_cse_perf(MCH_FILE, CORE_ROOT)\n",
    "\n",
    "def print_cse_info():\n",
    "    grouped_by_method = individual_cse.loc[individual_cse['viable']].groupby('method')\n",
    "\n",
    "    good_cses = []\n",
    "    good_cses_pct = []\n",
    "\n",
    "    heuristic_cses = []\n",
    "    heuristic_cses_pct = []\n",
    "    for _, method_info in grouped_by_method:\n",
    "        num_cses = len(method_info)\n",
    "        num_good = (method_info['diff'] < 0).sum()\n",
    "        good_cses.append(num_good)\n",
    "        good_cses_pct.append(num_good / num_cses)\n",
    "\n",
    "        # count method_infos with 'heuristic_selected' == True\n",
    "        num_heuristic = method_info['heuristic_selected'].sum()\n",
    "        heuristic_cses.append(num_heuristic)\n",
    "        heuristic_cses_pct.append(num_heuristic / num_cses)\n",
    "\n",
    "    print(f\"Avg good CSEs:   {np.mean(good_cses)}\")\n",
    "    print(f\"Avg good CSEs %: {np.mean(good_cses_pct) * 100:.2f}%\")\n",
    "    print()\n",
    "    print(f\"Avg heuristic CSEs:   {np.mean(heuristic_cses)}\")\n",
    "    print(f\"Avg heuristic CSEs %: {np.mean(heuristic_cses_pct) * 100:.2f}%\")\n",
    "\n",
    "print_cse_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Current CSE Heuristic (Linux x64)\n",
    "\n",
    "Let's next take a look at the quality of the current hand-written heuristic for selecting CSEs by comparing it to doing nothing at all (selecting no CSEs).  Our methodology here is simple:  JIT the method once with no CSEs enabled, then JIT the method with the JIT's default heuristic and see if the result is positive or negative.\n",
    "\n",
    "We run this experiment below.  As you can see, the Geomean of using no CSEs versus using the default heuristic on the 'libraries_tests_no_tiered_compilation' mch file is 0.98.  That is, using the heuristic is slightly better than using no CSEs at all, but not by a very large factor.  Digging in a little further, we find that the perfscore is the same 26% of the time, the heuristic is *worse* than using no CSEs 22% of the time, and the heuristic is better than choosing no CSEs 52% of the time.  The heuristic degrades performance by 5% when it chooses the wrong CSEs, and improves performance by 8% when it does so correctly.\n",
    "\n",
    "Essentially, when the current heuristic degrades performance slightly less than it improves it when it guesses correctly or incorrectly (+5% vs -8%), but it improves performance twice as often as it hurts it.  **[IMPORTANT]** This is a key finding, because much of my focus in improving the current JIT heursitic has been on finding CSEs to enable that will improve performance.  This research actually says that we can improve the JIT's current heuristic by finding places it enables a CSE when it shouldn't and preventing that.\n",
    "\n",
    "### Is it better than random chance?\n",
    "\n",
    "Another way to get a baseline of comparison is to generate random selections of CSE choices and see how well this does against our two methods (no-CSEs or the JIT heuristic).  We (of course) expect the JIT heuristic to be better than random choice, and we will certainly want any model we generate to also be better than random choice.  Let's validate that here.\n",
    "\n",
    "To do this, we take the average amount of CSEs (as a percentage of total CSEs).  We will then randomly select that many CSEs out of all viable CSEs for each method and compare it to the JIT's heuristic\n",
    "\n",
    "Knowing what we now know about CSEs (46% of them ar good, 54% of them are bad), the results versus enabling no CSEs is not surprising at all.  As expected, random selection of CSEs is roughly equal to choosing no CSEs at all.  The geomean between random selection and no CSEs at all is **1.00**.\n",
    "\n",
    "The comparison vs the JIT heuristic tells the same story with different numbers.  Random chance is worse than the current heuristic with a geomean of **1.01** compared to it.  The heuristic chooses better 62% of the time, and random chance chooses better CSEs than the heuristic 31% of the time.\n",
    "\n",
    "It's a bit surprsing that choosing random CSEs to enable outperforms the standard heuristic 31% of the time, but this turns out to be a common theme.  We will see a similar number below when we try to use a neural network classifier.\n",
    "\n",
    "The only notable thing about random chance vs the current JIT heuristic is the average improvement/degredation of performance.  On average, random chance improves performance by 4.4% when choosing correctly and degrades performance by 3.9% when it selects poorly.  This is notable because the default JIT heuristic degrades performance by 5.1% when it guesses wrong.  This again suggests that we may be able to find places where the JIT incorrectly enables bad CSEs for a big benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cse usage: 58.56%\n",
      "\n",
      "Geomean of heuristic vs no-cse: 0.98\n",
      "\n",
      "% of time same score:          25.73%\n",
      "% of time no-cse is better:    22.44%\n",
      "% of time heuristic is better: 51.83%\n",
      "\n",
      "Average improvement when heuristic is better: -7.75%\n",
      "Average degradation when no-cse is better:     5.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random choices: 100%|██████████████████████████████████████████████████████████| 131369/131369 [15:43<00:00, 139.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geomean of random vs no-cse: 1.00\n",
      "\n",
      "% of time same score:          3.10%\n",
      "% of time no-cse is better:    44.77%\n",
      "% of time random is better: 52.13%\n",
      "\n",
      "Average improvement when random is better: -4.39%\n",
      "Average degradation when no-cse is better:     3.85%\n",
      "\n",
      "Geomean of random vs heuristic: 1.01\n",
      "\n",
      "% of time same score:          7.59%\n",
      "% of time heuristic is better:    61.65%\n",
      "% of time random is better: 30.76%\n",
      "\n",
      "Average improvement when random is better: -3.31%\n",
      "Average degradation when heuristic is better:     3.78%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrapped as a method so we don't carry around these variables\n",
    "def print_cse_vs_heuristic_vs_random():\n",
    "    methods = []\n",
    "    heuristic_scores = []\n",
    "    no_cse_scores = []\n",
    "    pct_cses_used = []\n",
    "\n",
    "    # Loop through all methods in the .mch.  We will JIT each method with no CSEs enabled,\n",
    "    # discarding any methods which have no viable CSEs.  We will then JIT the method with\n",
    "    # the heuristic enabled and compare the perf_scores.\n",
    "    for method_id in cache.all_methods:\n",
    "        no_cse = cache.jit_method(spmi, method_id, MethodKind.NO_CSE)\n",
    "        if not any(True for candidate in no_cse.cse_candidates if candidate.viable):\n",
    "            continue\n",
    "\n",
    "        heuristic = cache.jit_method(spmi, method_id, MethodKind.HEURISTIC)\n",
    "\n",
    "        # If we got a perfscore of 0, ignore this method\n",
    "        if np.isclose(heuristic.perf_score, 0.0) or np.isclose(no_cse.perf_score, 0.0):\n",
    "            continue\n",
    "\n",
    "        heuristic_scores.append(heuristic.perf_score)\n",
    "        no_cse_scores.append(no_cse.perf_score)\n",
    "        pct_cses_used.append(heuristic.num_cse/ sum(1 for x in no_cse.cse_candidates if x.viable))\n",
    "        methods.append((no_cse, heuristic))\n",
    "\n",
    "    avg_cses_used = np.mean(pct_cses_used)\n",
    "    print(f\"Average cse usage: {avg_cses_used* 100:.2f}%\")\n",
    "    print()\n",
    "\n",
    "    print_difference(heuristic_scores, no_cse_scores, \"heuristic\", \"no-cse\")\n",
    "\n",
    "    # Now we will compare the heuristic to a random selection of CSEs\n",
    "    no_cse_baseline = []\n",
    "    heuristic_baseline = []\n",
    "    random_scores = []\n",
    "    for no_cse, heuristic in tqdm(methods, ncols=120, desc=\"Random choices\"):\n",
    "        viable = [x.index for x in no_cse.cse_candidates if x.viable]\n",
    "        count = int(len(viable) * avg_cses_used)\n",
    "        if count > 1:\n",
    "            choice = list(np.random.choice(viable, count, replace=False))\n",
    "            random = cache.jit_method(spmi, no_cse.index, choice)\n",
    "\n",
    "            # Unfortunately, we sometimes fail to JIT methods.  I haven't debugged this yet\n",
    "            if random is not None and not np.isclose(random.perf_score, 0.0):\n",
    "                random_scores.append(random.perf_score)\n",
    "                no_cse_baseline.append(no_cse.perf_score)\n",
    "                heuristic_baseline.append(heuristic.perf_score)\n",
    "\n",
    "\n",
    "    print_difference(random_scores, no_cse_baseline, \"random\", \"no-cse\")\n",
    "    print_difference(random_scores, heuristic_baseline, \"random\", \"heuristic\")\n",
    "\n",
    "print_cse_vs_heuristic_vs_random()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the Simplified Reinforcement Learning Model\n",
    "\n",
    "We previously built a reinforcement learning based model to attempt to tackle a simplified version of this problem.  This model has *several* limitations and is not meant to be the real solution that we can drop into the JIT.  Rather it was meant as a starting point to prove that the approach can work.  Here are the major limitations of the (intentionally simple) RL model we built:\n",
    "\n",
    "* It only works on methods with 3-16 CSE candidates (it cannot handle anything with more than 16 CSE candidates).\n",
    "* It cannot choose to enable no CSEs, even if all CSEs candidates are bad ones.\n",
    "* It JITs the method at every iteration to get the most recent info about CSE candidates.\n",
    "* The model can choose to apply CSEs which are already applied or are non-viable.\n",
    "* Using the model is somewhat complicated, because it requires us to handle the case where the model wants to apply a CSE which is already applied or non-viable.  That is, we have to make some choices in how this is handled and that decision making is outside of the model itself.\n",
    "\n",
    "Note that all of these limitations can be fixed and improved.  We simply built the most expedient model possible to prove that the approach works.\n",
    "\n",
    "With all of that said, it's still useful to compare this simplified model to the underlying JIT heuristic/no cse/random chance that we have before.\n",
    "\n",
    "### Setting up the Comparison\n",
    "\n",
    "The code that we have below requires that the reinforcement learning model we built be trained.  We do not check these models into the repo.  If you want to run this script below, you will need to train the model with:\n",
    "\n",
    "```bash\n",
    "./train.py models/rl/ ~/path/to/tests.mch --core_root ~/path/to/core_root --test-percent 0.2 --iterations N --parallel M\n",
    "```\n",
    "\n",
    "Where iterations is followed by the number of iterations to train for and the parallel flag is how many parallel processes to use (default is just 1).\n",
    "\n",
    "The model used below was trained for 10,000,000  iterations.\n",
    "\n",
    "### Results\n",
    "\n",
    "The output below shows that our reinforcement learning trained model performs very similarly on both the training data and test data it's never seen before.  For the text data, we have a `geomean` of 0.97 compared to methods with no CSEs selected.  We have a `geomean` of 0.99 when compared to the JIT's RL heuristic on these same methods.  So our reinforcement learning trained model performs slightly better than the current JIT heuristic on methods with up to 16 CSEs, despite this model's limitations.\n",
    "\n",
    "The RL trained model is better than random chance as well (1.02 geomean comparing random to the RL model).  The surprising finding here is that randomly selecting CSEs to enable is better than the RL model 31% of the time (same as the heuristic based approach).\n",
    "\n",
    "## Conclusions about an RL Approach\n",
    "\n",
    "While the particular model architecture we used in our first attempt at building a module using reinforcement learning isn't suitable for use in the JIT, it does show that this approach was successful.  For the limited scope of methods that this model handles, it performs better than the current JIT CSE heuristic.\n",
    "\n",
    "It's still worrisome that a third of the time, random chance selects better CSEs than this model, but this is no worse than the current CSE heuristic in the JIT.\n",
    "\n",
    "Before continuing down the path of using reinforcement learning to build a model for JIT CSE heuristics, we will first try to use some more standard and straightforward ML techniques, such as using neural networks to try to classify whether CSEs should be enabled or not.  If we find this simpler approach does not work, we can go back to investigating reinforcement learning based models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leculver/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training methods: 55261, Test methods: 6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating RL model: 100%|████████████████████████████████████████████████████████| 61401/61401 [29:10<00:00, 35.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failures: 104\n",
      "Average cse usage: 0.36\n",
      "\n",
      "Training data results:\n",
      "Geomean of rl vs no-cse: 0.97\n",
      "\n",
      "% of time same score:          9.78%\n",
      "% of time no-cse is better:    30.64%\n",
      "% of time rl is better: 59.59%\n",
      "\n",
      "Average improvement when rl is better: -5.90%\n",
      "Average degradation when no-cse is better:     1.77%\n",
      "\n",
      "Geomean of rl vs heuristic: 0.99\n",
      "\n",
      "% of time same score:          20.33%\n",
      "% of time heuristic is better:    36.88%\n",
      "% of time rl is better: 42.79%\n",
      "\n",
      "Average improvement when rl is better: -4.10%\n",
      "Average degradation when heuristic is better:     2.98%\n",
      "\n",
      "Test data results:\n",
      "Geomean of rl vs no-cse: 0.97\n",
      "\n",
      "% of time same score:          9.50%\n",
      "% of time no-cse is better:    31.01%\n",
      "% of time rl is better: 59.49%\n",
      "\n",
      "Average improvement when rl is better: -5.80%\n",
      "Average degradation when no-cse is better:     1.81%\n",
      "\n",
      "Geomean of rl vs heuristic: 0.99\n",
      "\n",
      "% of time same score:          19.55%\n",
      "% of time heuristic is better:    37.00%\n",
      "% of time rl is better: 43.45%\n",
      "\n",
      "Average improvement when rl is better: -4.08%\n",
      "Average degradation when heuristic is better:     2.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random choices: 100%|████████████████████████████████████████| 61285/61285 [06:30<00:00, 156.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geomean of rl vs random: 0.98\n",
      "\n",
      "% of time same score:          7.24%\n",
      "% of time random is better:    31.25%\n",
      "% of time rl is better: 61.51%\n",
      "\n",
      "Average improvement when rl is better: -3.77%\n",
      "Average degradation when random is better:     1.86%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jitml import JitCseModel, MethodContext\n",
    "from evaluate import get_most_likley_allowed_action\n",
    "\n",
    "def predict_cses(model : JitCseModel, method : MethodContext) -> MethodContext:\n",
    "    # We can't be done on the first iteration, this is a limitation of the current RL model.\n",
    "    # It also may look odd that we iterate over the method and jit it multiple times.  This\n",
    "    # also is how the simple RL model works, which is not how the next version of this model\n",
    "    # would be built if we continue down this path.\n",
    "    can_terminate = False\n",
    "    cses = []\n",
    "    while (action := get_most_likley_allowed_action(model, method, can_terminate)) is not None:\n",
    "        cses.append(action)\n",
    "        method = cache.jit_method(spmi, method.index, cses)\n",
    "        if method is None:\n",
    "            break\n",
    "\n",
    "        can_terminate = True\n",
    "        if not any(True for candidate in method.cse_candidates if candidate.viable):\n",
    "            break\n",
    "\n",
    "    return method\n",
    "\n",
    "# Implemented as a function so we don't carry these variables forward\n",
    "def evaluate_rl_model(algorithm, model_path):\n",
    "    model = JitCseModel(algorithm)\n",
    "    model.load(model_path)\n",
    "\n",
    "    failures = 0\n",
    "    rl_scores = {}\n",
    "    no_cse_scores = {}\n",
    "    heuristic_scores = {}\n",
    "    pct_cses = []\n",
    "    no_cse_methods = []\n",
    "\n",
    "    training_methods = set(cache.train_methods)\n",
    "    print(f\"Training methods: {len(training_methods)}, Test methods: {len(cache.test_methods)}\")\n",
    "\n",
    "    # This test/train split is for the RL model, which is different from the\n",
    "    # previous set of methods.  This is because the RL model can only handle\n",
    "    # methods with 3-16 CSEs.\n",
    "    all_methods = cache.test_methods + cache.train_methods\n",
    "    for method_index in tqdm(all_methods, ncols=120, desc=\"Evaluating RL model\"):\n",
    "        # JIT the method with no CSEs, and then JIT the method with the heuristic\n",
    "        no_cse = cache.jit_method(spmi, method_index, MethodKind.NO_CSE)\n",
    "        heuristic = cache.jit_method(spmi, method_index, MethodKind.HEURISTIC)\n",
    "\n",
    "        # JIT the method with the RL model\n",
    "        rl_result = predict_cses(model, no_cse)\n",
    "        if rl_result is None:\n",
    "            failures += 1\n",
    "            continue\n",
    "\n",
    "        if np.isclose(rl_result.perf_score, 0.0) or \\\n",
    "           np.isclose(no_cse.perf_score, 0.0) or \\\n",
    "           np.isclose(heuristic.perf_score, 0.0):\n",
    "            continue\n",
    "\n",
    "        rl_scores[method_index] = rl_result.perf_score\n",
    "        no_cse_scores[method_index] = no_cse.perf_score\n",
    "        heuristic_scores[method_index] = heuristic.perf_score\n",
    "        pct_cses.append(len(rl_result.cses_chosen) / sum(1 for x in no_cse.cse_candidates if x.viable))\n",
    "        no_cse_methods.append(no_cse)\n",
    "\n",
    "    avg_cses_used = np.mean(pct_cses)\n",
    "\n",
    "    print(f\"Failures: {failures}\")\n",
    "    print(f\"Average cse usage: {avg_cses_used:.2f}\")\n",
    "    print()\n",
    "\n",
    "    # Print out the results of the RL model on test/training data vs baselines\n",
    "    print(\"Training data results:\")\n",
    "    rl = [value for key, value in rl_scores.items() if key in training_methods]\n",
    "    no_cse = [value for key, value in no_cse_scores.items() if key in training_methods]\n",
    "    heuristic = [value for key, value in heuristic_scores.items() if key in training_methods]\n",
    "    print_difference(rl, no_cse, \"rl\", \"no-cse\")\n",
    "    print_difference(rl, heuristic, \"rl\", \"heuristic\")\n",
    "\n",
    "    print(\"Test data results:\")\n",
    "    rl = [value for key, value in rl_scores.items() if key not in training_methods]\n",
    "    no_cse = [value for key, value in no_cse_scores.items() if key not in training_methods]\n",
    "    heuristic = [value for key, value in heuristic_scores.items() if key not in training_methods]\n",
    "    print_difference(rl, no_cse, \"rl\", \"no-cse\")\n",
    "    print_difference(rl, heuristic, \"rl\", \"heuristic\")\n",
    "\n",
    "    # calculate random baseline\n",
    "    rl_for_random = []\n",
    "    random_scores = []\n",
    "    for no_cse in tqdm(no_cse_methods, ncols=100, desc=\"Random choices\"):\n",
    "        viable = [x.index for x in no_cse.cse_candidates if x.viable]\n",
    "        count = int(len(viable) * avg_cses_used)\n",
    "        if count > 1 and count <= len(viable):\n",
    "            choice = list(np.random.choice(viable, count, replace=False))\n",
    "            random = cache.jit_method(spmi, no_cse.index, choice)\n",
    "\n",
    "            # Sometimes JIT'ing fails, so we only consider successful JITs\n",
    "            if random is not None and not np.isclose(random.perf_score, 0.0):\n",
    "                random_scores.append(random.perf_score)\n",
    "                rl_for_random.append(rl_scores[no_cse.index])\n",
    "\n",
    "    print_difference(rl_for_random, random_scores, \"rl\", \"random\")\n",
    "\n",
    "evaluate_rl_model(MODEL_ALGORITHM, RL_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks for a single CSE Decision\n",
    "\n",
    "So reinforcement learning can work for this problem, but it's very difficult and complicated to train an RL model, and the one we have has a lot of limitations.  Let's take a look at some simpler approaches in deep learning.  We'll look at whether a neural network can predict whether enabling a CSE is a good or bad decision.  If we can do that, we may just be able to use it to tell us what CSEs to enable.\n",
    "\n",
    "We will use the `individual_cse_perf` dataset again.  This tells us a simplified story of what we are looking for:  Whether or not each individual CSE is a net improvement or detriment to overall perfscore when we enable it (and by what magnitude).  This is not the complete picture, since each CSE decision will affect every other CSE decision.  However, this is a good starting dataset for our model to see if it will even train, and if it trains then whether it makes for good CSE decision making.\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "Our input to the network will be a list of features that the JIT provides in `CSE_HeuristicRLHook::GetFeatures` (optcse.cpp).  We use scikit-learn's `StandardScaler` to scale the inputs effectively.  Some of the data (like the weights) might benefit from further thought into feature normalization.  Our output will be true/false of whether an individual CSE improves the perfscore or not (actually it's whether it improves the perfscore by at least `CSE_SUCCESS_THRESHOLD` as enabling a CSE that does nothing isn't helpful).  That means our output node will be a single neuron with `sigmoid` activation to get a probability.\n",
    "\n",
    "Since we are doing binary classification, our loss function will be `binary_crossentropy`, which is the standard choice for binary classification.  Similarly we will use the `adam` optimizer since it does a good job with this type of task, though this was chosen semi-arbitrarily.  Others like `RMSprop` would work well too.\n",
    "\n",
    "For our hidden layers, we've chosen 3 dense layers with 64 neurons each.  This choice of layers and neurons was chosen by experimenting with different architectures with [classification.py](../classification.py).  This program lets you specify the neural network architecture, optimizer, loss, etc and it will calculate the loss and accuracy of that network for this problem.\n",
    "\n",
    "Through experimenting we found that smaller networks of 8 neurons underfit the data, and 2-3 layers of 16, 32, or 64 neurons worked nicely.  `[64, 64, 64, 1]` produced the best result and didn't overfit, so we use it here.  Something like `[16, 16, 1]` is likely a more sensible choice with less parameters to train and had similar (but slightly worse) results.  We also experimented with mixed densities, like `[64, 32, 16, 1]` but they did not seem to provide any advantages.\n",
    "\n",
    "Below we train with 100 epochs since that seemed to give good results without seeing evidence of overfitting.  This number is also arbitrary, and can be increased to see how far we can push it, or decreased for a small penalty in accuracy.\n",
    "\n",
    "### A Quick Note about Test/Train split\n",
    "\n",
    "With this dataset we have to be careful about how we build our train/test split.  We cannot simply use sklearn's `train_test_split` to do this for us because multiple rows of data can come from the same method (each method can have multiple viable CSEs, after all).  We don't want to pollute the validation data with CSE decisions from methods that are in the training data.  This may not even be sufficient since some method bodies might be identical even if they came from different places.  We will have to fix that in a future version.\n",
    "\n",
    "### Single CSE Results\n",
    "\n",
    "As you can see from the output below, we hit ~98% accuracy on the training data and 97% accuracy on test data that the model has never seen before.  Meaning that for any individual CSE, this model can tell you with very high accuracy that just enabling that *one* cse and no others will have a positive or negative impact on perfscore.  (Actual numbers may vary from run to run, depending on the last time this script was run.)\n",
    "\n",
    "So, this model does successfully train and seems to have good results on test data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 291304 CSE decisions.\n",
      "Validating on 32401 CSE decisions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 14:38:16.398945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19444 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:65:00.0, compute capability: 8.9\n",
      "Epochs:   0%|                                                                                | 0/100 [00:00<?, ?epoch/s]2024-05-19 14:38:17.263968: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2024-05-19 14:38:17.264006: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2024-05-19 14:38:17.264125: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.304016: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.339277: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.370433: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.403925: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.440666: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.491903: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-05-19 14:38:17.515965: I external/local_xla/xla/service/service.cc:168] XLA service 0x71de8ad75f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-19 14:38:17.515987: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-05-19 14:38:17.520472: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-19 14:38:17.533871: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716154697.577204 1725885 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epochs: 100%|████████████████████████████████████████████| 100/100 [03:13<00:00,  1.94s/epoch, acc=0.979, val_acc=0.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy: loss:0.0473 accuracy:0.9808\n",
      "Test Accuracy:  loss:0.0751 accuracy:0.9740\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_len : int):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_len,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "def split_and_scale(df : DataFrame):\n",
    "    train_mask = df['method'].isin(cache.train_methods)\n",
    "    test_mask = df['method'].isin(cache.test_methods)\n",
    "    x_train, y_train = df[train_mask].drop(columns=['target', 'method']), df[train_mask]['target']\n",
    "    x_test, y_test = df[test_mask].drop(columns=['target', 'method']), df[test_mask]['target']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    return scaler, x_train, x_test, y_train, y_test, x_train.shape[1]\n",
    "\n",
    "# see notebooks/00_random_forest.ipynb for more information on the approach here\n",
    "def sanitize_data(df : DataFrame, threshold : float) -> DataFrame:\n",
    "    # Don't modify the original dataframe\n",
    "    result = df.copy()\n",
    "\n",
    "    # One-hot encode the type column\n",
    "    for member in JitType:\n",
    "        result[f\"type_{member.name}\"] = result['type'] == member\n",
    "\n",
    "    result.drop(columns=['type'], inplace=True)\n",
    "    result['selected'] = result['selected'].apply(len)\n",
    "    result['target'] = result['diff'] < threshold\n",
    "\n",
    "    # where result is viable\n",
    "    result = result[result['viable']]\n",
    "\n",
    "    # Drop columns we don't want to use as features\n",
    "    to_drop = ['cse_index', 'cse_score', 'no_cse_score', 'heuristic_score', 'heuristic_selected',\n",
    "               'index', 'applied', 'viable', 'diff']\n",
    "    to_drop = [x for x in to_drop if x in result.columns]\n",
    "\n",
    "    result.drop(columns=to_drop, inplace=True)\n",
    "    return result\n",
    "\n",
    "def train_single_cse_model(data : DataFrame, threshold : float):\n",
    "    # Loads a dataset that is all the CSEs in the .mch file individually JIT'ed.\n",
    "    normalized = sanitize_data(data, threshold)\n",
    "    scalar, x_train, x_test, y_train, y_test, feature_len = split_and_scale(normalized)\n",
    "\n",
    "    print(f\"Training on {len(x_train)} CSE decisions.\")\n",
    "    print(f\"Validating on {len(x_test)} CSE decisions.\")\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_model(feature_len)\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_test, y_test), verbose=0,\n",
    "              callbacks=[TqdmCallback(EPOCHS)])\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    print()\n",
    "    print(f\"Train Accuracy: loss:{train_loss:.4f} accuracy:{train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy:  loss:{test_loss:.4f} accuracy:{test_acc:.4f}\")\n",
    "\n",
    "    return scalar, model\n",
    "\n",
    "single_scalar, single_model = train_single_cse_model(individual_cse, CSE_SUCCESS_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does our Neural Network make for a better CSE heuristic?\n",
    "\n",
    "Just because the model trains and generalizes to test data doesn't mean that it will do better than the current heuristic.  If the output of the model were to be believed, we should be correctly predicting that CSE coinflip more than 95% of the time.  That's too good to be true.  We also trained from a dataset of individual CSE decisions, but CSEs work in concert with each other.  So let's test and see what the outcome actually is.\n",
    "\n",
    "Now that we have a trained model, we can take the CSEs for a method and use `model.predict` to tell us whether each CSE for the method is predicted to be below our success threshold.  We will use the same `SELECTION_PROBABILITY` as the default tensorflow criteria of 0.5.  That is, if the output of our neural network is above 0.5 we will select the CSE.\n",
    "\n",
    "We will then compare our neural network to JIT'ing the method with no CSEs enabled (which is very similar to random chance) and also compare it to the current JIT heuristic, hoping for a better outcome.\n",
    "\n",
    "### Results\n",
    "\n",
    "As you can see from the output below, the neural network we trained did not live up to our expectations.  Despite predicting the results of single CSE decisions with a high degree of accuracy, we unfortunately see that using this to make full CSE decisions for a method doesn't quite model reality.\n",
    "\n",
    "On the positive side, this model matched the current JIT CSE Heuristic (geomean of 1.00), though it does so in a very different way.  Because we set our selection threshold to be -5.0 perfscore or better, the model is very conservative.  We rarely make a \"wrong\" choice, but we choose very few CSEs overall.  (As an aside, this also gives more evidence that we may be able to improve the current heuristic by choosing less \"bad\" CSEs.)\n",
    "\n",
    "With this in mind, let's explore what happens when we be less strict with our threshold for a successful CSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from selecting CSEs with predicted perfscore < -5.0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting CSEs: 100%|██████████████████████████████████████████████████████████████| 6140/6140 [05:41<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CSEs chosen:   0.49\n",
      "Average CSEs chosen %: 8.82%\n",
      "\n",
      "VS No CSE\n",
      "Geomean of model vs no-cse: 0.98\n",
      "\n",
      "% of time same score:          77.00%\n",
      "% of time no-cse is better:    0.93%\n",
      "% of time model is better: 22.07%\n",
      "\n",
      "Average improvement when model is better: -11.10%\n",
      "Average degradation when no-cse is better:     1.42%\n",
      "\n",
      "\n",
      "VS Heuristic\n",
      "Geomean of model vs heuristic: 1.00\n",
      "\n",
      "% of time same score:          17.46%\n",
      "% of time heuristic is better:    48.85%\n",
      "% of time model is better: 33.69%\n",
      "\n",
      "Average improvement when model is better: -5.78%\n",
      "Average degradation when heuristic is better:     3.30%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SELECTION_PROBABILITY = 0.5\n",
    "\n",
    "def predict_cse_with_model(scalar, model, threshold):\n",
    "    model_scores = []\n",
    "    no_cse_scores = []\n",
    "    heuristic_scores = []\n",
    "    chosen = []\n",
    "    chosen_pct = []\n",
    "\n",
    "    grouped = individual_cse.groupby('method')\n",
    "    for method_id in tqdm(cache.test_methods, ncols=120, desc=\"Predicting CSEs\"):\n",
    "        # Sanitize data\n",
    "        if method_id not in grouped.groups:\n",
    "            continue\n",
    "\n",
    "        row = grouped.get_group(method_id)\n",
    "        row_viable = row[row['viable']]\n",
    "        features = sanitize_data(row_viable, threshold)\n",
    "        x = scalar.transform(features.drop(columns=['target', 'method']))\n",
    "\n",
    "        # Predict what CSEs to use\n",
    "        y_pred = model.predict(x, verbose=0).ravel()\n",
    "        above_threshold = np.where(y_pred > SELECTION_PROBABILITY)[0]\n",
    "        cses_chosen = above_threshold[np.argsort(-y_pred[above_threshold])].tolist()\n",
    "\n",
    "        # pull index out of row_viable\n",
    "        cses_chosen = [row_viable.iloc[x].cse_index for x in cses_chosen]\n",
    "\n",
    "        # JIT the method with the chosen CSEs\n",
    "        no_cse = cache.jit_method(spmi, method_id, MethodKind.NO_CSE)\n",
    "        heuristic = cache.jit_method(spmi, method_id, MethodKind.HEURISTIC)\n",
    "\n",
    "        if cses_chosen:\n",
    "            method = cache.jit_method(spmi, method_id, cses_chosen)\n",
    "            if method is None or np.isclose(method.perf_score, 0.0):\n",
    "                continue\n",
    "        else:\n",
    "            method = no_cse\n",
    "\n",
    "        model_scores.append(method.perf_score)\n",
    "        no_cse_scores.append(no_cse.perf_score)\n",
    "        heuristic_scores.append(heuristic.perf_score)\n",
    "        chosen.append(len(cses_chosen))\n",
    "        chosen_pct.append(len(cses_chosen) / sum(1 for x in no_cse.cse_candidates if x.viable))\n",
    "\n",
    "    print(f\"Average CSEs chosen:   {np.mean(chosen):.2f}\")\n",
    "    print(f\"Average CSEs chosen %: {np.mean(chosen_pct) * 100:.2f}%\")\n",
    "    print()\n",
    "    print(\"VS No CSE\")\n",
    "    print_difference(model_scores, no_cse_scores, \"model\", \"no-cse\")\n",
    "    print()\n",
    "    print(\"VS Heuristic\")\n",
    "    print_difference(model_scores, heuristic_scores, \"model\", \"heuristic\")\n",
    "\n",
    "print(f\"Results from selecting CSEs with predicted perfscore < {CSE_SUCCESS_THRESHOLD}:\")\n",
    "predict_cse_with_model(single_scalar, single_model, CSE_SUCCESS_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Second Attempt at an Individual CSE Based Neural Network\n",
    "\n",
    "In this attempt, we'll take the exact same neural network architecture and hyperparameters but instead try to train it to detect when there is any improvement in perfscore at all.  This uses 0 for our success threshold instead of -5.0.  We will expect this new model to select more CSEs, but also likely make more mistakes in choosing CSEs which regress perfscore.  We hope that, on balance, this will be a net improvement over the heuristic.\n",
    "\n",
    "One interesting side effect of setting the threshold to 0 is that the model is not as accurate on this data.  We hit 92% accuracy with this model (compared to 96% before).  This is most likely because many CSEs are clustered around the 0.0 perfscore change mark.  We might be able to increase the accuracy by using something somewhere between `[-5, 0]`, but at the end of the day the important thing is how well the model does on the more general scenario of picking multiple CSEs, not in training.\n",
    "\n",
    "The final outcome here is a better one than the previous model, despite the lower accuracy on the training data.  We managed to beat the current JIT heuristic by a non-trivial margin, with a geomean of 0.98.  This still isn't a huge win though, just incremental progress.\n",
    "\n",
    "One interesting finding here is that the CSEs chosen has risen in this model up to 46%.  This is the exact percentage of time that we found individual CSEs to be beneficial.  This is a good sign that our model is well calibrated to the training data we gave it.  That gives us good hope that if we can feed a neural network better data and more relevant features, we should see even better results.  Hopefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 291304 CSE decisions.\n",
      "Validating on 32401 CSE decisions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|████████████████████████████████████████████| 100/100 [03:34<00:00,  2.14s/epoch, acc=0.931, val_acc=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy: loss:0.1769 accuracy:0.9315\n",
      "Test Accuracy:  loss:0.2143 accuracy:0.9213\n",
      "\n",
      "Results from selecting CSEs with predicted perfscore < 0.0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting CSEs: 100%|██████████████████████████████████████████████████████████████| 6140/6140 [06:42<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CSEs chosen:   2.31\n",
      "Average CSEs chosen %: 45.67%\n",
      "\n",
      "VS No CSE\n",
      "Geomean of model vs no-cse: 0.96\n",
      "\n",
      "% of time same score:          25.59%\n",
      "% of time no-cse is better:    3.00%\n",
      "% of time model is better: 71.41%\n",
      "\n",
      "Average improvement when model is better: -6.66%\n",
      "Average degradation when no-cse is better:     1.41%\n",
      "\n",
      "\n",
      "VS Heuristic\n",
      "Geomean of model vs heuristic: 0.98\n",
      "\n",
      "% of time same score:          28.64%\n",
      "% of time heuristic is better:    5.42%\n",
      "% of time model is better: 65.94%\n",
      "\n",
      "Average improvement when model is better: -3.86%\n",
      "Average degradation when heuristic is better:     1.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NEW_THRESHOLD = 0.0\n",
    "scalar_zero, model_zero = train_single_cse_model(individual_cse, NEW_THRESHOLD)\n",
    "\n",
    "print()\n",
    "print(f\"Results from selecting CSEs with predicted perfscore < {NEW_THRESHOLD}:\")\n",
    "predict_cse_with_model(scalar_zero, model_zero, NEW_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts and Conclusions\n",
    "\n",
    "We took a wide walk through CSE decision making in this notebook.  We were able to build three ML models which are as good or better than the current JIT heuristic:  The reinforcement learning trained model we built a few weeks past, and two classification models based on the `individual_cse` dataset.\n",
    "\n",
    "Our first attempt at building a classic neural network (without RL) to match the JIT heuristic.  However, it does not generalize as well as we expected it to based on its classification success.  Our second attempt was not as accurate on the test or training data, but resulted in a better overall model which better fit the real-world scenario...though still nowhere near as good as we might hope.\n",
    "\n",
    "We may be able to train a better network in the future by giving it data about multiple CSE selections instead of \"individual ones\".  One of our next steps will be trying to generate better training data from multiple CSE decisions.\n",
    "\n",
    "Here are some other quick takeaways:\n",
    "\n",
    "* Individual CSE decisions are essentially a coinflip, 46% of the time an individual CSE will be a positive change in perfscore (if no others are enabled).\n",
    "* This \"individual\" CSE decision data doesn't seem to completely generalize to selecting multiple CSEs.\n",
    "* The current hand-written JIT heuristic does quite well, but is *worse* than choosing no CSEs (or random chance) 26% of the time.\n",
    "* We may be able to improve the hand-written JIT by eliminating places where it guesses wrong instead of trying to find new places to say yes.\n",
    "* The initial simplified reinforcement learning trained model we built does do better than the current JIT heuristic for functions with 3-16 CSEs.\n",
    "* The `individual_cse` dataset can be learned and generalized by a neural network.\n",
    "* This resulting neural network trained with `CSE_SUCCESS_THRESHOLD = -5.0` does better than the current JIT heuristic and our simplified RL model.  However, it does not live up to its \"97% success\" rate when it goes from picking individual CSEs to a series of them.\n",
    "* We can change our `CSE_SUCCESS_THRESHOLD` to 0 for better performance.  The neural network guesses individual CSE perfscore less accurately, but the resulting network generalizes better to selecting CSEs for a full method.\n",
    "\n",
    "Lastly, note that this is another step in a series of explorations.  This is not the final outcome of this work.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The next project here is to build a better dataset of CSE decisions and results.  We need to be careful to not double-count methods with the same method body, such that that data leaks between the training and test set.  I think this can be accomplished with the method hash that comes from the JIT.  Once that's complete we can try to build a better, more real model using that data.\n",
    "\n",
    "Here are a series of other follow ups from this work:\n",
    "\n",
    "* Build a better dataset that incorporates multiple CSE decisions.\n",
    "* Attempt to train a new neural network on a multi-cse dataset and see if it's better or worse.\n",
    "* Attempt to identify bad CSE decisions by the current JIT heuristic and see if we can update the heuristic to work better.\n",
    "* Take a closer look at the features the JIT provides for CSE decisions.  What changes after we enable a CSE?\n",
    "* Consider adding features for basic block data (like variables live across the block).\n",
    "* Attempt to build a regression model to predict high value CSEs, instead of just the true/false classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
